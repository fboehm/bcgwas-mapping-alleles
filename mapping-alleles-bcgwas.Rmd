---
title: "BCGWAS: Mapping alleles from bcgwas website and gtexportal"
author: "Fred Boehm"
date: "December 16, 2014"
output: 
  html_document:
    toc: true
    toc_depth: 4
---

## Revision to original strategy

My earlier idea was to download annotation files from illumina.com. However, I was unsure which files corresponded to our chip for bcgwas. It then occurred to me that some of the originally downloaded files, which Michael got from the Washington University genotyping facility, might have the allele mappings.

Note that we'll also need to see how I coded the alleles when creating the numeric genotypes.

```{r, "setup"}
library(knitr)
library(stringr)
opts_knit$set(progress = TRUE, verbose = TRUE, cache=TRUE)
```

## Downloading annotation files from www.biostat.wisc.edu/~ngroup/bcgwas/SNPB/

```{r}
DATA_DIR <- "../bcgwas-ngroup/data"
```

We downloaded the file "SNP_Data_Gould_Infinium_1M_Omni/SNP_Map.csv"

Now, we read that file into R.

```{r, cache=TRUE, eval=FALSE}
# 'eval=FALSE' means that we don't execute this chunk
smap1<- read.csv(file.path(DATA_DIR,"SNP_Map.csv")) 
head(smap1)
```

It seems that the SNP_Map.csv file contains  insufficient. I'll check the other file, "ForwardStrand_FinalReport.csv"

```{r, cache=TRUE}

smap<- read.csv(file.path(DATA_DIR,"ForwardStrand_FinalReport.csv"), skip=9, colClasses= "character")
names(smap)[1] <- "snp.id"
head(smap)
```

```{r, cache=TRUE, eval=FALSE}
smap2<- read.csv(file.path(DATA_DIR, "test.csv"), skip=9) 
# test.csv is the "head" (first ~50 lines ) of ForwardStrand_FinalReport.csv
head(smap2)
```


Once we have the mapping from A/B to A/C/T/G, we can create a small genotype file from the raw csv file and compare it with the geno.ord.RData file. Note that we need to do this to ensure that we know how we created geno.ord.RData as far as mapping A/B to 0,1,2 genotype classes. It may be that I didn't translate the ACTG to Illumina's AB; rather, I might have arbitrarily chosen an allele as the "A" and then counted for each genotype, the number of "A" alleles.

This raises a bigger question -- can I reproduce the pre-processing steps in the bcgwas analysis?

**I'd like to try to regenerate geno.ord.RData from the source files.**

# Dec 24, 2014

## Reading text file from ngroup/snp directory


## Reading in geno.ord.RData 

We are trying to re-create the geno.ord.RData file. We want to compare a portion the current geno.ord.RData file with the file that we'd get from smap object. We also load the snp annotation object that we created in august 2011.

```{r "reading-geno.ord", cache=TRUE, eval=TRUE}
load("../bcgwas-data/EIGENBCGWAS_Data/geno.ord.RData") # loads geno.ord
library(GWASTools)
library(stringr)# for subsequent manipulations of actg genotypes
sann<- pData(getobj("../bcgwas-data/SNP_annotation/illumina.madison.snp.annot.v3.RData"))
```

```{r "make-newdat", cache=TRUE, eval=TRUE}
newdat <- smap[,c(-11, -23, -32, -33)]# remove the samples that have more than 1 run each.
```

We then removed all four runs involving N31 or N57 since I don't recall which - N31 or N31.rep and N57.rep or N57 - was used. I'm quite certain that the ".rep" columns were used, but I don't recall whether these two are part of the correlation issue.

We then 

```{r, "genoparse-defined"}

genoparse <- function(gv.actg, # gv.actg is a character vector with each entry a 2-letter genotype
                       sid=names(gv.actg) # sid is the vector of subject IDs
                      )  
{
  foo<- strsplit(gv.actg, "")
  f2<- matrix(ncol = 2, data=unlist(foo), byrow=TRUE)
  o1  <- apply(FUN = function(x)sum(x==f2[1]), X = f2, MARGIN=1)
  # define two alleles, "other" & "ref"
  other<- f2[f2 != f2[1]][1]
  ref <- f2[1]
  o2 <- c(ref, other)
  names(o2)<- c("ref", "oth")
  names(o1)<- sid
  names(gv.actg)<- sid
  out1<- list(f2, o1, o2)
  names(out1)<- c("input_geno", "genovec", "allele_identities")
  o4 <- c(other, ref)
  names(o4)<- c("ref", "oth")
  o3 <- 2-o1
  out2 <- list(f2, o3, o4)
  names(out2)<- c("input_geno", "genovec", "allele_identities")  
  out <- list(out1, out2)
  return(out)
  }

```

Now, try genoparse function.

```{r}
t1 <- c("TT", "TC", "CC", "CC")
names(t1)<- paste0("subj", 1:4)
genoparse(t1)
```

Next steps: 

1. function to compare the genotype vector from geno.ord with that outputted by genoparse()
2. after comparing, decide which allele is mapped to 1 and which to zero in geno.ord.


```{r "matchalleles-defined"}
matchalleles <- function(rs.id, 
                   g.ord = geno.ord
                   , 
                   gm.actg = newdat
                   )
  {
  # get subject ids
  sid<- intersect(names(gm.actg), colnames(g.ord))
  d1<- str_extract(as.character(gm.actg[gm.actg$snp.id ==rs.id,colnames(gm.actg)%in% sid]), "[A-Z]+")
  gpout<- genoparse(as.character(d1))
  go.vec<- as.vector(g.ord[rownames(g.ord) == rs.id, colnames(g.ord) %in% sid])
  out1 <- data.frame(gpout[[1]]$input_geno, t(go.vec), gpout[[1]]$genovec, gpout[[2]]$genovec)
  # define out2 as the mapping that outputs geno.ord's codings
  if (allequal(out1[,3], out1[,4])) out2 <- gpout[[1]]$allele_identities
  if (allequal(out1[,3], out1[,5])) out2 <- gpout[[2]]$allele_identities
  out3 <- c(paste0(out2[1], out2[1]), paste0(out2[1], out2[2]), paste0(out2[2], out2[2]))
  names(out3)<- c("geno.2", "geno.1", "geno.0")
  out <- list(out1, out2, out3)
  return(out) # out is a list. first entry is a df with 4 columns (one subject per row): 1. genotype.actg, 2. genotype.numeric (from geno.ord), 3. genotype.actg translation #1, 4. genotype.actg translation #2. 
  }
```

Now, let's use the userfn function.

```{r}
rs <- "rs2710875"
matchalleles(rs)
```


Todo: revise genoparse() so that out[[1]]$input_geno is only the ACTG genotypes DONE!

Todo: mapping from actg genotypes to 1000 genomes

A search for "1000 genomes"  on bioconductor.org site pointed me to ceu1kg package. Let's install it.

```{r "install-ceu1kg", eval=FALSE}
source("http://bioconductor.org/biocLite.R")
biocLite("ceu1kg")
biocLite("SNPlocs.Hsapiens.dbSNP.20101109")
```

Now, let's look at it.

```{r}
library(ceu1kg)
dir(system.file("parts", package="ceu1kg"))
lk = load(dir(system.file("parts", package="ceu1kg"),full=TRUE)[1])
c1gt = get(lk)
c1gt
```





```{r}
# Consider removing this chunk! Note that it uses an old version of SNPlocs
library(SNPlocs.Hsapiens.dbSNP.20101109)
if (!exists("c1loc")) c1loc = getSNPlocs("ch1", as.GRanges=TRUE)
c1loc
rsn1 = paste("rs", elementMetadata(c1loc)$RefSNP_id, sep="")
length(intersect(rsn1, colnames(c1gt)))
ext1 = grep("chr", colnames(c1gt))
ext1 = as.numeric(gsub("chr1:", "", colnames(c1gt)[ext1]))
length(intersect(ext1, start(c1loc)))
```



```{r}
data(eset) # assume ceu1kg is first in line, yields ex in global
c1m = c1gt[sampleNames(ex),]
c1ss = make_smlSet( ex, list(chr1=c1m) )
c1ss
```

```{r "sessionInfo"}
sessionInfo()
```


We initially sought to read the vcf file with read.delim. 

```{r, eval=FALSE}
vc <- read.delim("ALL.BCM.20101123.snps.low_coverage.sites.vcf")
```

We examine the top of the file using head from bash: 


head ALL.BCM.20101123.snps.low_coverage.sites.vcf


Below, we need to verify that the reference allele gets put first (and alternate second) when creating the gds file. 

GWASTools has functions to work with gdsfmt files and can convert vcf file to gds format. 

```{r, "using-gwastools", cache=TRUE}
library(GWASTools)
#convertVcfGds("ALL.wg.NCBI.20101123.snps.sites.vcf", "ALL.wg.NCBI.20101123.snps.sites.gds") 
onekg<- read.table("1kg-data/ALL.wg.NCBI.20101123.snps.sites.vcf")
```



NOTE: 
Since the downloaded vcf file (from one thousand genomes website) doesn't have rsids, we'll get them from the dbsnp package on bioconductor.

```{r "install-dbsnp", eval = FALSE}
source("http://bioconductor.org/biocLite.R")
biocLite("SNPlocs.Hsapiens.dbSNP141.GRCh38")
```

The above code failed, saying that the package was available as a source, but not as a binary. So, I downloaded the source and installed from command line. I initially tried to use install.packages() within an R session, but I got an error that the file is not a mac osx binary package. I then used R CMD INSTALL from the bash command line, and that installed it.


#### Mapping from rsid's to chromosome & position

We treat the first 

```{r}
library(SNPlocs.Hsapiens.dbSNP141.GRCh38)
# remove col 8 from "onekg"
onekga <- onekg[,1:7]
names(onekga)[c(1:2,4:5)]<- 
```

Can we use readVcf() instead of read.table?

```{r}
library(VariantAnnotation)
seqinfo("1kg-data/ALL.BCM.20101123.snps.low_coverage.sites.vcf")
readVcf("1kg-data/test.vcf")
```


## Using GTEx_genot_imputed_variants_info4_maf05_CR95_CHR_POSb37_ID_REF_ALT.txt

We had downloaded on December 11, 2014 the file GTEx_genot_imputed_variants_info4_maf05_CR95_CHR_POSb37_ID_REF_ALT.txt.zip. Today (2 January 2015) we unzipped it. In using 'head' from bash shell, it looks like it is the map that we need to complete our work. Note that wc (wordcount) from bash shell showed it to have more than 6 million lines.

```{r}
gt<- read.table("../gtex/testg")
names(gt)<- c("Chrom", "Pos", "rs.id", "ref.allele", "alt.allele")
```



```{r}
sessionInfo()
```


# Other considerations

- Use eQTL package
- Use vignette here: http://master.bioconductor.org/help/workflows/eQTL/


